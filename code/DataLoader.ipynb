{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"hHeslB1lHxn4"}},{"cell_type":"code","source":["import os\n","from os.path import join\n","import time\n","import h5py\n","from sentence_transformers import SentenceTransformer\n","from PIL import Image\n","import torch\n","from torchvision import transforms"],"metadata":{"id":"7wxNVk9xHuLE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Downloading Data"],"metadata":{"id":"6FoXhoX3MgTB"}},{"cell_type":"code","source":["# flower dataset download link: http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\n","# upload \"jpg\" folder to Drive\n","\n","# caption dataset download link: https://drive.google.com/uc?export=download&confirm=l7Ld&id=0B0ywwgffWnLLcms2WWJQRFNSWXM\n","# upload \"text_c10\" folder to Drive"],"metadata":{"id":"BDa1O-LhMkDG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Google Drive Setup"],"metadata":{"id":"0VTyr81nMbeb"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"id":"uhLBnGXMMOGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datadir = \"/content/drive/My Drive/CS444/Final_Project\"\n","os.chdir(datadir)\n","!pwd"],"metadata":{"id":"Xl85AnSmMPXL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Caption Embeddings"],"metadata":{"id":"ziDv5gbZPI6W"}},{"cell_type":"code","source":["# caption saving code based on: https://github.com/paarthneekhara/text-to-image/blob/master/data_loader.py\n","\n","def save_caption_vectors_flowers(datadir):\n","    # flower image directory\n","    img_dir = join(datadir, 'jpg')\n","    # get all jpgs\n","    image_files = [f for f in os.listdir(img_dir) if 'jpg' in f]\n","    # num_samples\n","    print(len(image_files))\n","\n","    # initialize dictionary: key = img file, value = captions\n","    image_captions = {img_file: [] for img_file in image_files}\n","\n","    caption_dir = join(datadir, 'text_c10')\n","    class_dirs = []\n","    # 102 class subdirectories (class_00001, ..., class_00102).\n","    for i in range(1, 103):\n","        class_dir_name = 'class_%.5d' % (i)\n","        class_dirs.append(join(caption_dir, class_dir_name))\n","\n","    # read all .txt caption files for each folder\n","    for class_dir in class_dirs:\n","        caption_files = [f for f in os.listdir(class_dir) if 'txt' in f]\n","        for cap_file in caption_files:\n","            with open(join(class_dir, cap_file)) as f:\n","                captions = f.read().split('\\n')\n","            # reconstruct image filename from caption file name\n","            img_file = cap_file[0:11] + \".jpg\"\n","            # add 5 captions for each image\n","            image_captions[img_file] += [cap for cap in captions if len(cap) > 0][0:5]\n","\n","    # confirm every image has captions\n","    print(\"images with captions:\", len(image_captions))\n","\n","    # load best text encoder based on evaluation\n","    # text_encoder = 'all-MiniLM-L6-v2'\n","    # text_encoder = 'multi-qa-mpnet-base-dot-v1'\n","    text_encoder = datadir + '/text_encoders/finetuned10_multi-qa-mpnet-base-dot-v1'\n","    model = SentenceTransformer(text_encoder)\n","\n","    encoded_captions = {}\n","\n","    # loop over every image and encode its captions\n","    for i, img in enumerate(image_captions):\n","        # model.encode takes a list of strings and returns (n_captions, embed_dim)\n","        encoded_captions[img] = model.encode(image_captions[img])\n","\n","    # Save the encoded caption vectors to an HDF5 file.\n","    # h = h5py.File(join(datadir, 'basic_encoded_captions.hdf5'), 'w')\n","    # h = h5py.File(join(datadir, 'advanced_encoded_captions.hdf5'), 'w')\n","    h = h5py.File(join(datadir, 'finetuned_encoded_captions.hdf5'), 'w')\n","    for key in encoded_captions:\n","        h.create_dataset(key, data=encoded_captions[key])\n","    h.close()\n","\n","save_caption_vectors_flowers(datadir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56fXFx37PKBt","executionInfo":{"status":"ok","timestamp":1746326741318,"user_tz":300,"elapsed":1335423,"user":{"displayName":"Dhruv Saligram","userId":"03145468349522013786"}},"outputId":"a57a045f-1ab0-43ac-b138-6ee206e05cde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8189\n","images with captions: 8189\n"]}]},{"cell_type":"markdown","source":["# Save Resized Images To Disk In Tensor"],"metadata":{"id":"SQltxcWg3RJg"}},{"cell_type":"code","source":["image_dir = datadir + '/jpg'\n","cache_path = datadir + 'image_cache.pt'\n","\n","to_tensor = transforms.ToTensor()\n","\n","# load all images into a dict of tensors\n","tensor_cache = {}\n","for fname in os.listdir(image_dir):\n","    if not fname.lower().endswith('.jpg'):\n","        continue\n","    img = Image.open(os.path.join(image_dir, fname)).convert('RGB')\n","    # resize images before saving to reduce RAM use during computation + storage\n","    img = img.resize((64, 64), Image.BILINEAR)\n","    tensor_cache[fname] = to_tensor(img)\n","\n","# save to drive\n","torch.save(tensor_cache, cache_path)"],"metadata":{"id":"ToiYZS_a3TgB"},"execution_count":null,"outputs":[]}]}