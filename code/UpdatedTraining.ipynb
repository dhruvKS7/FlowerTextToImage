{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1guI7zitlj1GP_UdN5GV6_dyL_qPAYN25","timestamp":1746593549702},{"file_id":"1qvsA4OWLpbWDQWdYTQ8E6qGgWbek18Pd","timestamp":1746592443370},{"file_id":"1CDEdrEqQ7vLMUqsw92l901eVR0d6wSDK","timestamp":1746472744356}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"js5bxmM1H6uQ"}},{"cell_type":"code","source":["import os\n","from os.path import join\n","import random\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils import spectral_norm\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import time\n","import h5py\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt"],"metadata":{"id":"gKhVLeUVH8Nl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Google Drive Setup"],"metadata":{"id":"0VTyr81nMbeb"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"id":"uhLBnGXMMOGi","executionInfo":{"status":"ok","timestamp":1746593777832,"user_tz":300,"elapsed":17413,"user":{"displayName":"Dhruv Saligram","userId":"03145468349522013786"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7185ab30-e679-46f8-aaac-49fbf350267a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["datadir = \"/content/drive/My Drive/CS444/Final_Project\"\n","os.chdir(datadir)\n","!pwd"],"metadata":{"id":"Xl85AnSmMPXL","executionInfo":{"status":"ok","timestamp":1746593778332,"user_tz":300,"elapsed":501,"user":{"displayName":"Dhruv Saligram","userId":"03145468349522013786"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"75b4e88f-c03c-41b2-8ab9-477c38def7e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/CS444/Final_Project\n"]}]},{"cell_type":"markdown","source":["# Training Parameters"],"metadata":{"id":"i2cnaAN3MijB"}},{"cell_type":"code","source":["opt = {\n","    'large': 0,           # flag for adding extra blocks to generator\n","    'save_every': 100,    # save models and optimizers during training every X epochs\n","    'print_every': 15,    # print statistics every X batch per epoch\n","    'cls_weight': 0.5,    # weight for wrong image/text pairs\n","    'checkpoint_dir': datadir + '/checkpoints', # where to save models and optimizers\n","    'captions_file': datadir + '/base_encoded_captions.hdf5', # where dataset captions were stored in DataLoader\n","    'cache_path': datadir + '/image_cache.pt', # where dataset images were stored in DataLoader\n","    'fine_size': 64,      # size of cached images saved in DataLoader\n","    'batch_size': 64,     # number of items per batch\n","    'txt_size': 384,      # dimensions of text embeddings (based on encoder used)\n","    'nc': 3,              # image channels (3 for RGB)\n","    'nt': 256,            # dimensions of text features\n","    'nz': 100,            # dimensions for noise\n","    'ngf': 128,           # number of generator filters in first conv layer\n","    'ndf': 64,            # number of discriminator filters in first conv layer\n","    'num_workers': 2,     # workers for data loader\n","    'epochs': 600,        # number of training epochs\n","    'lr': 0.0002,         # init learning rate for Adam optimizer\n","    'lr_decay': 0.5,      # learning rate decay factor\n","    'decay_every': 100,   # learning rate decay frequency\n","    'beta1': 0.5,         # momentum term of Adam\n","    'train_amt': 0.75,    # percent of dataset for training (train/test split)\n","    'display': 1,         # flag whether to display sample every epoch while training (0 = False)\n","    'noise': 'normal',    # noise type: \"uniform\" or \"normal\"\n","    'init_g': '',         # path to saved generator\n","    'init_d': '',         # path to saved discriminator\n","    'init_g_opt': '',     # path to saved generator optimizer\n","    'init_d_opt': '',     # path to saved discriminator optimizer\n","    'resume': 0,          # flag whether to resume training from saved models\n","    'manual_seed': 7,     # manual seed for reproducible results\n","}"],"metadata":{"id":"oo_ndPJa1CjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialization Setup"],"metadata":{"id":"lbBZ9TB7-NY9"}},{"cell_type":"code","source":["# set device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# set seed\n","random.seed(opt['manual_seed'])\n","torch.manual_seed(opt['manual_seed'])\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(opt['manual_seed'])\n","\n","# set default type\n","torch.set_default_dtype(torch.float32)"],"metadata":{"id":"R8jT3d0WyeHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generator Definition"],"metadata":{"id":"IQlu2gHXqroz"}},{"cell_type":"code","source":["# NOTE: removed all inplace=True tags due to runtime errors\n","\n","# reimplementation of ConcatTable & CAddTable block in original generator code\n","# applies conv branch and elementwise adds the identity\n","class ConcatAddBlock(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels):\n","        super(ConcatAddBlock, self).__init__()\n","        self.conv_branch = nn.Sequential(\n","            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(hidden_channels),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(hidden_channels),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return x + self.conv_branch(x)\n","\n","# generator definition\n","# as close to 1:1 reimplementation as possible\n","class Generator(nn.Module):\n","    def __init__(self, opt):\n","        super(Generator, self).__init__()\n","        self.nz = opt['nz']\n","        self.nt = opt['nt']\n","        self.txt_size = opt['txt_size']\n","        self.ngf = opt['ngf']\n","        self.nc = opt['nc']\n","        self.large = opt['large']\n","\n","        # transformation for text embedding\n","        self.fcG = nn.Sequential(\n","            nn.Linear(self.txt_size, self.nt),\n","            nn.LeakyReLU(0.2)\n","        )\n","\n","        self.deconv1 = nn.ConvTranspose2d(self.nz + self.nt, self.ngf * 8, kernel_size=4, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.ngf * 8)\n","\n","        # state size: (ngf*8) x 4 x 4\n","        self.resblock1 = ConcatAddBlock(self.ngf * 8, self.ngf * 2, self.ngf * 8)\n","        if self.large == 1:\n","            self.resblock1b = ConcatAddBlock(self.ngf * 8, self.ngf * 2, self.ngf * 8)\n","\n","        # upsample from 4x4 to 8x8\n","        self.deconv2 = nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(self.ngf * 4)\n","\n","        # state size: (ngf*4) x 8 x 8\n","        self.resblock2 = ConcatAddBlock(self.ngf * 4, self.ngf, self.ngf * 4)\n","        if self.large == 1:\n","            self.resblock2b = ConcatAddBlock(self.ngf * 4, self.ngf, self.ngf * 4)\n","\n","        # upsample from 8x8 to 16x16\n","        self.deconv3 = nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.ngf * 2)\n","\n","        # upsample from 16x16 to 32x32\n","        self.deconv4 = nn.ConvTranspose2d(self.ngf * 2, self.ngf, kernel_size=4, stride=2, padding=1, bias=False)\n","        self.bn4 = nn.BatchNorm2d(self.ngf)\n","\n","        # upsample from 32x32 to 64x64\n","        self.deconv5 = nn.ConvTranspose2d(self.ngf, self.nc, kernel_size=4, stride=2, padding=1, bias=False)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, noise, txt):\n","        batch_size = noise.size(0)\n","        # process text through fcG\n","        txt_out = self.fcG(txt)\n","        # reshape to (batch, nt, 1, 1)\n","        txt_out = txt_out.view(batch_size, self.nt, 1, 1)\n","\n","        # concatenate noise and processed text\n","        # shape: (batch, nz + nt, 1, 1)\n","        input_vec = torch.cat([noise, txt_out], dim=1)\n","\n","        x = self.deconv1(input_vec)\n","        x = self.bn1(x)\n","\n","        x = self.resblock1(x)\n","        if self.large == 1:\n","            x = self.resblock1b(x)\n","        x = F.relu(x)\n","\n","        x = self.deconv2(x)\n","        x = self.bn2(x)\n","\n","        x = self.resblock2(x)\n","        if self.large == 1:\n","            x = self.resblock2b(x)\n","        x = F.relu(x)\n","\n","        x = self.deconv3(x)\n","        x = self.bn3(x)\n","        x = F.relu(x)\n","\n","        x = self.deconv4(x)\n","        x = self.bn4(x)\n","        x = F.relu(x)\n","\n","        x = self.deconv5(x)\n","        output = self.tanh(x)\n","        return output"],"metadata":{"id":"V_5RbpTU8F9V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Discriminator Definition"],"metadata":{"id":"_3Px1oZfqxhf"}},{"cell_type":"code","source":["# NOTE: removed all inplace=True tags due to runtime errors\n","\n","# reimplementation of Replicate in original discriminator code\n","# replicates a vector spatially to (HxW) feature map\n","class Replicate2d(nn.Module):\n","    def __init__(self, H, W):\n","        super(Replicate2d, self).__init__()\n","        self.H = H\n","        self.W = W\n","\n","    def forward(self, x):\n","        x = x.reshape(x.size(0), x.size(1), 1, 1)\n","        return x.repeat(1, 1, self.H, self.W)\n","\n","# image branch of discriminator\n","class ImageDiscriminator(nn.Module):\n","    def __init__(self, nc, ndf):\n","        super(ImageDiscriminator, self).__init__()\n","        # input is (nc) x 64 x 64\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=True),\n","            nn.LeakyReLU(0.2)\n","        )\n","        # state size: (ndf) x 32 x 32\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2)\n","        )\n","        # state size: (ndf*2) x 16 x 16\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 4),\n","            nn.LeakyReLU(0.2)\n","        )\n","        # state size: (ndf*4) x 8 x 8\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 8)\n","        )\n","        # state size: (ndf*8) x 4 x 4\n","        self.res_branch = nn.Sequential(\n","            nn.Conv2d(ndf * 8, ndf * 2, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(ndf * 2, ndf * 2, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 2),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(ndf * 2, ndf * 8, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(ndf * 8)\n","        )\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        identity = x\n","        out_branch = self.res_branch(x)\n","        # elementwise sum (ConcatTable + CAddTable)\n","        x = identity + out_branch\n","        x = F.leaky_relu(x, 0.2)\n","        return x\n","\n","# text branch of discriminator\n","class TextDiscriminator(nn.Module):\n","    def __init__(self, txt_size, nt):\n","        super(TextDiscriminator, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(txt_size, nt),\n","            nn.LeakyReLU(0.2)\n","        )\n","        self.replicate = Replicate2d(4, 4)\n","\n","    def forward(self, txt):\n","        # (batch, nt)\n","        x = self.fc(txt)\n","        # (batch, nt, 4, 4)\n","        x = self.replicate(x)\n","        return x\n","\n","# discriminator definition (combines previous discriminators)\n","class Discriminator(nn.Module):\n","    def __init__(self, opt):\n","        super(Discriminator, self).__init__()\n","        self.nc = opt['nc']\n","        self.ndf = opt['ndf']\n","        self.nt = opt['nt']\n","        self.txt_size = opt['txt_size']\n","\n","        self.image_net = ImageDiscriminator(self.nc, self.ndf)\n","        self.text_net = TextDiscriminator(self.txt_size, self.nt)\n","\n","        # combined features have shape (ndf*8 + nt)\n","        self.joint_conv1 = nn.Sequential(\n","            nn.Conv2d(self.ndf * 8 + self.nt, self.ndf * 8, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(self.ndf * 8),\n","            nn.LeakyReLU(0.2)\n","        )\n","        self.joint_conv2 = nn.Sequential(\n","            nn.Conv2d(self.ndf * 8, 1, kernel_size=4, stride=1, padding=0, bias=True),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, img, txt):\n","        # process image and text branches separately\n","        # (batch, ndf*8, 4, 4)\n","        img_feat = self.image_net(img)\n","        # (batch, nt, 4, 4)\n","        txt_feat = self.text_net(txt)\n","        # concatenate\n","        # (batch, ndf*8 + nt, 4, 4)\n","        joint = torch.cat([img_feat, txt_feat], dim=1)\n","        # (batch, ndf*8, 4, 4)\n","        x = self.joint_conv1(joint)\n","        # (batch, 1, 1, 1)\n","        x = self.joint_conv2(x)\n","        # reshape to (batch, 1)\n","        x = x.reshape(x.size(0), -1)\n","        return x"],"metadata":{"id":"wzftwIFq9WfK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generator & Discriminator Creation"],"metadata":{"id":"8r1zCVeg-fnG"}},{"cell_type":"code","source":["# weight initialization function taken from original code\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if 'Conv' in classname:\n","        if hasattr(m, 'weight') and m.weight is not None:\n","            nn.init.normal_(m.weight, 0.0, 0.02)\n","        if hasattr(m, 'bias') and m.bias is not None:\n","            nn.init.constant_(m.bias, 0)\n","    elif 'BatchNorm' in classname:\n","        if hasattr(m, 'weight') and m.weight is not None:\n","            nn.init.normal_(m.weight, 1.0, 0.02)\n","        if hasattr(m, 'bias') and m.bias is not None:\n","            nn.init.constant_(m.bias, 0)\n","\n","# define generator\n","if opt['init_g'] == '':\n","    netG = Generator(opt).to(device)\n","    netG.apply(weights_init)\n","else:\n","    netG = torch.load(opt['init_g'])\n","\n","# define discriminator\n","if opt['init_d'] == '':\n","    netD = Discriminator(opt).to(device)\n","    netD.apply(weights_init)\n","else:\n","    netD = torch.load(opt['init_d'])\n","\n","# define optimizers\n","if opt['init_g_opt'] == '':\n","    optimizerG = torch.optim.Adam(netG.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n","else:\n","    optimizerG = torch.optim.Adam(netG.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n","    optimizerG.load_state_dict(torch.load(opt['init_g_opt']))\n","\n","if opt['init_d_opt'] == '':\n","    optimizerD = torch.optim.Adam(netD.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n","else:\n","    optimizerD = torch.optim.Adam(netD.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n","    optimizerD.load_state_dict(torch.load(opt['init_d_opt']))"],"metadata":{"id":"vFCUdoBo-hYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adding spectral normalization to discriminator\n","for name, module in netD.named_modules():\n","    if isinstance(module, nn.Conv2d):\n","        spectral_norm(module)"],"metadata":{"id":"1TjjFxOm9AOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Preparation"],"metadata":{"id":"cuSsaDf_q228"}},{"cell_type":"code","source":["# define loss criterion\n","criterion = nn.BCELoss()\n","criterion = criterion.to(device)\n","\n","# create tensors for inputs and labels\n","input_img = torch.empty(opt['batch_size'], opt['nc'], opt['fine_size'], opt['fine_size'])\n","input_img2 = torch.empty(opt['batch_size'], opt['nc'], opt['fine_size'], opt['fine_size'])\n","input_txt_emb1 = torch.empty(opt['batch_size'], opt['txt_size'])\n","noise = torch.empty(opt['batch_size'], opt['nz'], 1, 1)\n","\n","input_img = input_img.to(device)\n","input_img2 = input_img2.to(device)\n","input_txt_emb1 = input_txt_emb1.to(device)\n","noise = noise.to(device)\n","\n","# create global variables for error values so they can be referenced in all methods\n","errD = None\n","errG = None\n","errW = None\n","\n","# labels for training\n","real_label = 1.0\n","fake_label = 0.0\n","\n","# define fake score\n","fake_score = 0.5"],"metadata":{"id":"V_49yqtg-bwe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Loading"],"metadata":{"id":"mtVsMx74q6OB"}},{"cell_type":"code","source":["def load_data():\n","    h = h5py.File(opt['captions_file'])\n","    flower_captions = {}\n","    for key, ds in h.items():\n","        flower_captions[key] = np.array(ds)\n","    images = [key for key in flower_captions]\n","    images.sort()\n","    images_train = int(len(images) * opt['train_amt'])\n","    # make sure that number of train images splits perfectly into batches\n","    if images_train % opt['batch_size'] != 0:\n","        images_train += opt['batch_size'] - (images_train % opt['batch_size'])\n","    training_images = images[0:images_train]\n","    random.shuffle(training_images)\n","\n","    return {\n","\t\t    'image_list' : training_images,\n","\t\t\t  'captions' : flower_captions\n","\t\t}\n","\n","loaded_data = load_data()"],"metadata":{"id":"KLyD_l_wDzfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transformation to be applied every time an image is retrieved\n","transform = transforms.Compose([\n","    transforms.Resize(74),\n","    transforms.RandomCrop(64),\n","    transforms.RandomHorizontalFlip(0.5),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","class FlowerTextImageDataset(Dataset):\n","    def __init__(self, cache_path, captions_dict, image_list, transform):\n","        \"\"\"\n","        cache_path: cached file mapping name to raw image data\n","        captions_dict: mappping name to caption array\n","        image_list: list of names (keys for both cache_path & captions_dict)\n","        transform: torchvision transforms to apply each call\n","        \"\"\"\n","        super().__init__()\n","        self.cache = torch.load(cache_path)\n","        self.captions = captions_dict\n","        self.image_list = image_list\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        # get real image\n","        fname_real = self.image_list[idx]\n","        data_real = self.cache[fname_real]\n","        # convert to uint8 (H, W, C) np for PIL\n","        arr_real = (data_real * 255).byte().cpu()\n","        arr_real = arr_real.permute(1, 2, 0).numpy()\n","        img_real = Image.fromarray(arr_real)\n","        real_img_tensor = self.transform(img_real)\n","\n","        # randomly select 1 caption for real image\n","        caps = self.captions[fname_real]\n","        cap_idx = random.randrange(len(caps))\n","        txt_emb = torch.from_numpy(caps[cap_idx]).float()\n","\n","        # ensure wrong_idx != idx\n","        wrong_idx = random.randint(0, len(self.image_list) - 2)\n","        if wrong_idx >= idx:\n","            wrong_idx += 1\n","        # get wrong image\n","        fname_wrong = self.image_list[wrong_idx]\n","        data_wrong = self.cache[fname_wrong]\n","        # convert to uint8 (H, W, C) np for PIL\n","        arr_wrong = (data_wrong * 255).byte().cpu()\n","        arr_wrong = arr_wrong.permute(1, 2, 0).numpy()\n","        img_wrong = Image.fromarray(arr_wrong)\n","        wrong_img_tensor = self.transform(img_wrong)\n","\n","        return real_img_tensor, wrong_img_tensor, txt_emb\n","\n","# create dataset\n","dataset = FlowerTextImageDataset(\n","    cache_path=opt['cache_path'],\n","    captions_dict=loaded_data['captions'],\n","    image_list=loaded_data['image_list'],\n","    transform=transform\n",")\n","\n","# create dataloader\n","loader = DataLoader(\n","    dataset,\n","    batch_size=opt['batch_size'],\n","    shuffle=True,\n","    num_workers=opt['num_workers'],\n","    pin_memory=True\n",")"],"metadata":{"id":"D5i2Tpu7-gvp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Closures"],"metadata":{"id":"b-7ely3drSQW"}},{"cell_type":"code","source":["# generator closure\n","def fGx():\n","    # reference global variables used in function\n","    global fake_score, input_img, input_img2, input_txt_emb1, noise, netD, netG, criterion, optimizerG\n","\n","    # zero biases in all conv layers for both netD and netG\n","    for m in netD.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)) and m.bias is not None:\n","            m.bias.data.zero_()\n","    for m in netG.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)) and m.bias is not None:\n","            m.bias.data.zero_()\n","\n","    optimizerG.zero_grad()\n","\n","    # generate random noise\n","    if opt['noise'] == 'uniform':\n","        noise.uniform_(-1, 1)\n","    elif opt['noise'] == 'normal':\n","        noise.normal_(0, 1)\n","\n","    # generate fake images from netG\n","    fake = netG(noise, input_txt_emb1)\n","    # copy into global tensor\n","    fake.detach()\n","    input_img = fake.type(torch.float32).to(device)\n","    # for generator loss, use real label\n","    label_real = torch.full((input_img.shape[0], 1), real_label, dtype=torch.float32, device=device)\n","\n","    # compute netD output on the fake images\n","    output = netD(input_img, input_txt_emb1)\n","    # update fake score\n","    cur_score = output.mean().item()\n","    fake_score = 0.99 * fake_score + 0.01 * cur_score\n","\n","    errG = criterion(output, label_real)\n","\n","    errG.backward()\n","\n","    return errG.item()"],"metadata":{"id":"aCKbLVK1_IzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# discriminator closure\n","def fDx():\n","    # reference global variables used in function\n","    global fake_score, input_img, input_img2, input_txt_emb1, noise, netD, netG, criterion, optimizerD\n","\n","    # zero biases in all conv layers for both netD and netG\n","    for m in netD.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)) and m.bias is not None:\n","            m.bias.data.zero_()\n","    for m in netG.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)) and m.bias is not None:\n","            m.bias.data.zero_()\n","\n","    optimizerD.zero_grad()\n","\n","    # train with real examples\n","    label_real = torch.full((input_img.shape[0], 1), real_label, dtype=torch.float32, device=device)\n","    output = netD(input_img, input_txt_emb1)\n","    # label = label.view_as(output)\n","    errD_real = criterion(output, label_real)\n","\n","    # train with wrong image/text pairs\n","    errD_wrong = 0.0\n","    if opt['cls_weight'] > 0:\n","        label_fake = torch.full((input_img.shape[0], 1), fake_label, dtype=torch.float32, device=device)\n","        output_wrong = netD(input_img2, input_txt_emb1)\n","        errD_wrong = opt['cls_weight'] * criterion(output_wrong, label_fake)\n","\n","    # train with fake examples\n","    # generate random noise\n","    if opt['noise'] == 'uniform':\n","        noise.uniform_(-1, 1)\n","    elif opt['noise'] == 'normal':\n","        noise.normal_(0, 1)\n","\n","    # generate fake images from netG\n","    fake = netG(noise, input_txt_emb1)\n","    # copy into global tensor\n","    fake.detach()\n","    input_img = fake.type(torch.float32).to(device)\n","\n","    label_fake2 = torch.full((input_img.shape[0], 1), fake_label, dtype=torch.float32, device=device)\n","\n","    # compute netD output on the fake images\n","    output_fake = netD(input_img, input_txt_emb1)\n","    # update fake score\n","    cur_score = output_fake.mean().item()\n","    fake_score = 0.99 * fake_score + 0.01 * cur_score\n","\n","    fake_weight = 1 - opt['cls_weight']\n","    errD_fake = criterion(output_fake, label_fake2) * fake_weight\n","\n","    # calculate total loss for the discriminator\n","    errD = errD_real + errD_wrong + errD_fake\n","    errD.backward()\n","\n","    return errD.item(), errD_real.item(), errD_wrong.item(), errD_fake.item()"],"metadata":{"id":"LUr_W7M4A9lB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"ICRWwBQArQgC"}},{"cell_type":"code","source":["# calculate number of batches\n","total_batches = math.floor(len(dataset) / opt['batch_size'])\n","\n","# create fixed noise and label for generator display every epoch\n","fixed_z = torch.randn(1, opt['nz'], 1, 1, device=device)\n","idx = opt['manual_seed'] % len(dataset)\n","_, _, txt = dataset[idx]\n","fixed_txt = txt.float().to(device)\n","\n","# add noise starting from this epoch\n","start_noise = 20\n","# for this many epochs\n","noise_epochs = 50\n","# linearly decay from this to 0\n","initial_sigma = 0.3\n","\n","# determine which epoch to start at\n","start = 1\n","if opt['resume'] == 1:\n","    start = int(opt['init_g'].split('/')[-1].split('_')[0])\n","\n","for epoch in range(start, opt['epochs'] + 1):\n","    netG.train()\n","    netD.train()\n","\n","    # determine whether to add noise and how much\n","    if epoch < (start_noise + noise_epochs) and epoch >= start_noise:\n","        sigma = initial_sigma * max(0.0, ((start_noise + noise_epochs) - epoch) / noise_epochs)\n","    else:\n","        sigma = 0\n","\n","    epoch_start_time = time.time()\n","\n","    # decay the learning rate at the specified interval\n","    if epoch % opt['decay_every'] == 0:\n","        for param_group in optimizerG.param_groups:\n","            param_group['lr'] *= opt['lr_decay']\n","        for param_group in optimizerD.param_groups:\n","            param_group['lr'] *= opt['lr_decay']\n","\n","    for i, (real_images, wrong_images, captions) in enumerate(loader):\n","        iter_start_time = time.time()\n","\n","        input_img = real_images.float().to(device)\n","        input_img2 = wrong_images.float().to(device)\n","        input_txt_emb1 = captions.float().to(device)\n","\n","        # apply one-sided label smoothing\n","        real_label = round(random.uniform(0.85, 1.00), 2)\n","        # fake_label = round(random.uniform(0.00, 0.20), 2)\n","\n","        if sigma > 0:\n","            input_img += sigma * torch.randn_like(input_img)\n","            input_img2 += sigma * torch.randn_like(input_img2)\n","\n","        # discriminator gradients and loss\n","        errD, errDreal, errW, errDfake = fDx()\n","        optimizerD.step()\n","\n","        # generator gradients and loss\n","        # do X G updates for every discriminator update\n","        for _ in range(1):\n","            errG = fGx()\n","            optimizerG.step()\n","\n","        # log batch statistics\n","        if (i % opt['print_every']) == 0:\n","            iter_time = time.time() - iter_start_time\n","            current_lr = optimizerG.param_groups[0]['lr']\n","            print(f\"[{epoch}][{i}/{total_batches}] T:{iter_time:.3f} lr:{current_lr:.4g} \"\n","                  f\"G:{errG if errG is not None else -1:.3f}  D:{errD if errD is not None else -1:.3f}  \"\n","                  f\"Dr:{errDreal if errDreal is not None else -1:.3f}  Df:{errDfake if errDfake is not None else -1:.3f}  \"\n","                  f\"W:{errW if errW is not None else -1:.3f}  fs:{fake_score:.2f}\")\n","\n","    # display sample generation\n","    if opt['display'] == 1:\n","        netG.eval()\n","        with torch.no_grad():\n","            fake = netG(fixed_z, fixed_txt)\n","        # compute statistics\n","        mn, mx = fake.min().item(), fake.max().item()\n","        mean, std = fake.mean().item(), fake.std().item()\n","        print(\"\")\n","        print(f\"[{epoch}]  generator: min {mn:.4f}, max {mx:.4f}, mean {mean:.4f}, std {std:.4f}\")\n","        print(\"\")\n","        # convert for plotting\n","        img = (fake[0].cpu() + 1) * 0.5\n","        img = img.permute(1, 2, 0).numpy()\n","\n","        # visualize\n","        plt.figure(figsize=(3,3))\n","        plt.imshow(img)\n","        plt.axis('off')\n","        plt.title(f\"Epoch {epoch}\")\n","        plt.show()\n","        print(\"\")\n","        netG.train()\n","\n","    # save checkpoints at intervals\n","    if epoch % opt['save_every'] == 0:\n","        os.makedirs(opt['checkpoint_dir'], exist_ok=True)\n","        # save model & optimizer state dicts\n","        torch.save(netG.state_dict(), f\"{opt['checkpoint_dir']}/{epoch}_net_G_basic_updated.pth\")\n","        torch.save(netD.state_dict(), f\"{opt['checkpoint_dir']}/{epoch}_net_D_basic_updated.pth\")\n","        torch.save(optimizerG.state_dict(), f\"{opt['checkpoint_dir']}/{epoch}_opt_G_basic_updated.pth\")\n","        torch.save(optimizerD.state_dict(), f\"{opt['checkpoint_dir']}/{epoch}_opt_D_basic_updated.pth\")\n","    epoch_duration = time.time() - epoch_start_time\n","    print(f\"End of epoch {epoch} / {opt['epochs']} \\t Time Taken: {epoch_duration:.3f}\")\n","    print(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1p8VSB7l9v19F9DRUIpcbYAcYIMl_qCZr"},"id":"DMLwXmRGVt8f","executionInfo":{"status":"ok","timestamp":1746609665209,"user_tz":300,"elapsed":15647989,"user":{"displayName":"Dhruv Saligram","userId":"03145468349522013786"}},"outputId":"b829da6f-c7e7-4e47-add2-9f8b19cfdbfd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}