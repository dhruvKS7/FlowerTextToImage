{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations & Imports"
      ],
      "metadata": {
        "id": "QYwP8pM473Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "INzjJcukjEyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import h5py\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from os.path import join\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qVSla2SYjAUt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive Setup"
      ],
      "metadata": {
        "id": "8SQgcqFN76Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgOLH9aii4XT",
        "outputId": "5752b434-30b4-4ad9-947f-bd47bd18148e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = \"/content/drive/My Drive/CS444/Final_Project\"\n",
        "os.chdir(datadir)\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsPeVy7ti5tW",
        "outputId": "ba9d9626-096d-42f7-be6c-8f7f79f9f401"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/CS444/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Parameter Setup"
      ],
      "metadata": {
        "id": "teRGPes478UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\n",
        "    'large': 0,           # flag for adding extra blocks to generator\n",
        "    'save_every': 100,    # save models and optimizers during training every X epochs\n",
        "    'print_every': 15,    # print statistics every X batch per epoch\n",
        "    'cls_weight': 0.5,    # weight for wrong image/text pairs\n",
        "    'checkpoint_dir': datadir + '/checkpoints', # where to save models and optimizers\n",
        "    'captions_file': datadir + '/base_encoded_captions.hdf5', # where dataset captions were stored in DataLoader\n",
        "    'cache_path': datadir + '/image_cache.pt', # where dataset images were stored in DataLoader\n",
        "    'fine_size': 64,      # size of cached images saved in DataLoader\n",
        "    'batch_size': 64,     # number of items per batch\n",
        "    'txt_size': 384,      # dimensions of text embeddings (based on encoder used)\n",
        "    'nc': 3,              # image channels (3 for RGB)\n",
        "    'nt': 256,            # dimensions of text features\n",
        "    'nz': 100,            # dimensions for noise\n",
        "    'ngf': 128,           # number of generator filters in first conv layer\n",
        "    'ndf': 64,            # number of discriminator filters in first conv layer\n",
        "    'num_workers': 2,     # workers for data loader\n",
        "    'epochs': 600,        # number of training epochs\n",
        "    'lr': 0.0002,         # init learning rate for Adam optimizer\n",
        "    'lr_decay': 0.5,      # learning rate decay factor\n",
        "    'decay_every': 100,   # learning rate decay frequency\n",
        "    'beta1': 0.5,         # momentum term of Adam\n",
        "    'train_amt': 0.75,    # percent of dataset for training (train/test split)\n",
        "    'display': 1,         # flag whether to display sample every epoch while training (0 = False)\n",
        "    'noise': 'normal',    # noise type: \"uniform\" or \"normal\"\n",
        "    'init_g': datadir + '/checkpoints/600_net_G_classic.pth',         # path to saved generator\n",
        "    'init_d': '',         # path to saved discriminator\n",
        "    'init_g_opt': '',     # path to saved generator optimizer\n",
        "    'init_d_opt': '',     # path to saved discriminator optimizer\n",
        "    'resume': 0,          # flag whether to resume training from saved models\n",
        "    'manual_seed': 7,     # manual seed for reproducible results\n",
        "}"
      ],
      "metadata": {
        "id": "D9fZVdS3i6rB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "JGUrdxWY8AO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# set seed\n",
        "random.seed(opt['manual_seed'])\n",
        "torch.manual_seed(opt['manual_seed'])\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(opt['manual_seed'])\n",
        "\n",
        "# set default type\n",
        "torch.set_default_dtype(torch.float32)"
      ],
      "metadata": {
        "id": "Lwk6XxGwi8rY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator Definition"
      ],
      "metadata": {
        "id": "Fq0kI76B8Bru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: removed all inplace=True tags due to runtime errors\n",
        "\n",
        "# reimplementation of ConcatTable & CAddTable block in original generator code\n",
        "# applies conv branch and elementwise adds the identity\n",
        "class ConcatAddBlock(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(ConcatAddBlock, self).__init__()\n",
        "        self.conv_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_branch(x)\n",
        "\n",
        "# generator definition\n",
        "# as close to 1:1 reimplementation as possible\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = opt['nz']\n",
        "        self.nt = opt['nt']\n",
        "        self.txt_size = opt['txt_size']\n",
        "        self.ngf = opt['ngf']\n",
        "        self.nc = opt['nc']\n",
        "        self.large = opt['large']\n",
        "\n",
        "        # transformation for text embedding\n",
        "        self.fcG = nn.Sequential(\n",
        "            nn.Linear(self.txt_size, self.nt),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(self.nz + self.nt, self.ngf * 8, kernel_size=4, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.ngf * 8)\n",
        "\n",
        "        # state size: (ngf*8) x 4 x 4\n",
        "        self.resblock1 = ConcatAddBlock(self.ngf * 8, self.ngf * 2, self.ngf * 8)\n",
        "        if self.large == 1:\n",
        "            self.resblock1b = ConcatAddBlock(self.ngf * 8, self.ngf * 2, self.ngf * 8)\n",
        "\n",
        "        # upsample from 4x4 to 8x8\n",
        "        self.deconv2 = nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(self.ngf * 4)\n",
        "\n",
        "        # state size: (ngf*4) x 8 x 8\n",
        "        self.resblock2 = ConcatAddBlock(self.ngf * 4, self.ngf, self.ngf * 4)\n",
        "        if self.large == 1:\n",
        "            self.resblock2b = ConcatAddBlock(self.ngf * 4, self.ngf, self.ngf * 4)\n",
        "\n",
        "        # upsample from 8x8 to 16x16\n",
        "        self.deconv3 = nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.ngf * 2)\n",
        "\n",
        "        # upsample from 16x16 to 32x32\n",
        "        self.deconv4 = nn.ConvTranspose2d(self.ngf * 2, self.ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(self.ngf)\n",
        "\n",
        "        # upsample from 32x32 to 64x64\n",
        "        self.deconv5 = nn.ConvTranspose2d(self.ngf, self.nc, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, noise, txt):\n",
        "        batch_size = noise.size(0)\n",
        "        # process text through fcG\n",
        "        txt_out = self.fcG(txt)\n",
        "        # reshape to (batch, nt, 1, 1)\n",
        "        txt_out = txt_out.view(batch_size, self.nt, 1, 1)\n",
        "\n",
        "        # concatenate noise and processed text\n",
        "        # shape: (batch, nz + nt, 1, 1)\n",
        "        input_vec = torch.cat([noise, txt_out], dim=1)\n",
        "\n",
        "        x = self.deconv1(input_vec)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        x = self.resblock1(x)\n",
        "        if self.large == 1:\n",
        "            x = self.resblock1b(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.deconv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x = self.resblock2(x)\n",
        "        if self.large == 1:\n",
        "            x = self.resblock2b(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.deconv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.deconv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.deconv5(x)\n",
        "        output = self.tanh(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "SBKgQywIjmSa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Installation"
      ],
      "metadata": {
        "id": "h43cykxz8DjN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PnZIAIHJKo25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22d00d6-cc26-4f6f-ea90-172be84ce05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [01:20<00:00, 4.39MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load CLIP\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Transformation for generated images\n",
        "clip_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                         (0.26862954, 0.26130258, 0.27577711)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data Split"
      ],
      "metadata": {
        "id": "7jtbMMw18NE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your test split\n",
        "def get_test_split():\n",
        "    h = h5py.File(opt['captions_file'])\n",
        "    flower_captions = {}\n",
        "    for key, ds in h.items():\n",
        "        flower_captions[key] = np.array(ds)\n",
        "    images = sorted(flower_captions)\n",
        "    images_train = int(len(images) * opt['train_amt'])\n",
        "    images_train += opt['batch_size'] - (images_train % opt['batch_size']) if images_train % opt['batch_size'] != 0 else 0\n",
        "    training_images = images[0:images_train]\n",
        "    test_images = images[images_train:]\n",
        "    image_names = set(test_images)\n",
        "    print(len(test_images))\n",
        "    print(test_images[0])\n",
        "    caption_dir = join(datadir, 'text_c10')\n",
        "    class_dirs = []\n",
        "    # 102 class subdirectories (class_00001, ..., class_00102).\n",
        "    for i in range(1, 103):\n",
        "        class_dir_name = 'class_%.5d' % (i)\n",
        "        class_dirs.append(join(caption_dir, class_dir_name))\n",
        "\n",
        "    image_captions = {}\n",
        "\n",
        "    # read all .txt caption files for each folder\n",
        "    for class_dir in class_dirs:\n",
        "        caption_files = [f for f in os.listdir(class_dir) if 'txt' in f]\n",
        "        for cap_file in caption_files:\n",
        "            if cap_file[0:11] + '.jpg' not in image_names:\n",
        "                continue\n",
        "            with open(join(class_dir, cap_file)) as f:\n",
        "                captions = f.read().split('\\n')\n",
        "            # reconstruct image filename from caption file name\n",
        "            img_file = cap_file[0:11] + '.jpg'\n",
        "            # add 5 captions for each image\n",
        "            image_captions[img_file] = [cap for cap in captions if len(cap) > 0][0:5]\n",
        "    print(len(image_captions))\n",
        "    return test_images, image_captions\n",
        "\n",
        "test_images, flower_captions = get_test_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBUguAwmlNwb",
        "outputId": "69af76a5-3ee4-4267-8d63-0ec30f1e85d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2045\n",
            "image_06145.jpg\n",
            "2045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring"
      ],
      "metadata": {
        "id": "U8z1EGsv8O2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with classic\n",
        "# Load netG\n",
        "netG = Generator(opt).to(device)\n",
        "netG.load_state_dict(torch.load(opt['init_g']))\n",
        "netG.eval()\n",
        "\n",
        "clip_scores = []\n",
        "\n",
        "for img_id in test_images:\n",
        "    # Randomly select 1 of the 5 captions\n",
        "    all_captions = flower_captions[img_id]\n",
        "    selected_idx = random.randint(0, 4)\n",
        "    selected_caption_text = str(all_captions[selected_idx])\n",
        "\n",
        "    # Load corresponding sentence embedding\n",
        "    h = h5py.File(opt['captions_file'])\n",
        "    caption_vector = np.array(h[img_id])[selected_idx][:opt['txt_size']]\n",
        "    h.close()\n",
        "\n",
        "    # Generate noise + embed caption\n",
        "    z = np.random.normal(-1, 1, (1, opt['nz'], 1, 1)).astype(np.float32)\n",
        "    z_tensor = torch.tensor(z, device=device)\n",
        "    caption_tensor = torch.tensor(caption_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_img = netG(z_tensor, caption_tensor)\n",
        "\n",
        "    # Post-process image: (1, 3, H, W) → (H, W, 3) in [0, 255]\n",
        "    fake_img_np = fake_img[0].cpu().numpy()\n",
        "    fake_img_np = ((fake_img_np + 1) * 127.5).astype(np.uint8)\n",
        "    fake_img_np = np.transpose(fake_img_np, (1, 2, 0))\n",
        "\n",
        "    # Convert and preprocess for CLIP\n",
        "    img_clip = clip_transform(fake_img_np).unsqueeze(0).to(device)\n",
        "    text_clip = clip.tokenize([selected_caption_text]).to(device)\n",
        "\n",
        "    # Get CLIP embeddings\n",
        "    with torch.no_grad():\n",
        "        img_feat = clip_model.encode_image(img_clip)\n",
        "        txt_feat = clip_model.encode_text(text_clip)\n",
        "\n",
        "    # Normalize and compute cosine similarity\n",
        "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
        "    similarity = (img_feat @ txt_feat.T).item()\n",
        "    clip_scores.append(similarity)\n",
        "\n",
        "# Final result\n",
        "average_clip_score = np.mean(clip_scores)\n",
        "print(f\"Average CLIP cosine similarity over test set: {average_clip_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpzl4cryc_kI",
        "outputId": "54df7fc0-fed9-4c10-c493-dfce269f34a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average CLIP cosine similarity over test set: 0.2874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with classic_updated\n",
        "# Load netG\n",
        "netG = Generator(opt).to(device)\n",
        "netG.load_state_dict(torch.load(opt['init_g']))\n",
        "netG.eval()\n",
        "\n",
        "clip_scores = []\n",
        "\n",
        "for img_id in test_images:\n",
        "    # Randomly select 1 of the 5 captions\n",
        "    all_captions = flower_captions[img_id]\n",
        "    selected_idx = random.randint(0, 4)\n",
        "    selected_caption_text = str(all_captions[selected_idx])\n",
        "\n",
        "    # Load corresponding sentence embedding\n",
        "    h = h5py.File(opt['captions_file'])\n",
        "    caption_vector = np.array(h[img_id])[selected_idx][:opt['txt_size']]\n",
        "    h.close()\n",
        "\n",
        "    # Generate noise + embed caption\n",
        "    z = np.random.normal(-1, 1, (1, opt['nz'], 1, 1)).astype(np.float32)\n",
        "    z_tensor = torch.tensor(z, device=device)\n",
        "    caption_tensor = torch.tensor(caption_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_img = netG(z_tensor, caption_tensor)\n",
        "\n",
        "    # Post-process image: (1, 3, H, W) → (H, W, 3) in [0, 255]\n",
        "    fake_img_np = fake_img[0].cpu().numpy()\n",
        "    fake_img_np = ((fake_img_np + 1) * 127.5).astype(np.uint8)\n",
        "    fake_img_np = np.transpose(fake_img_np, (1, 2, 0))\n",
        "\n",
        "    # Convert and preprocess for CLIP\n",
        "    img_clip = clip_transform(fake_img_np).unsqueeze(0).to(device)\n",
        "    text_clip = clip.tokenize([selected_caption_text]).to(device)\n",
        "\n",
        "    # Get CLIP embeddings\n",
        "    with torch.no_grad():\n",
        "        img_feat = clip_model.encode_image(img_clip)\n",
        "        txt_feat = clip_model.encode_text(text_clip)\n",
        "\n",
        "    # Normalize and compute cosine similarity\n",
        "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
        "    similarity = (img_feat @ txt_feat.T).item()\n",
        "    clip_scores.append(similarity)\n",
        "\n",
        "# Final result\n",
        "average_clip_score = np.mean(clip_scores)\n",
        "print(f\"Average CLIP cosine similarity over test set: {average_clip_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycgkGieQtEuC",
        "outputId": "d87fabac-f053-4c99-8768-814c68bb289e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average CLIP cosine similarity over test set: 0.2882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with updated\n",
        "# Load netG\n",
        "netG = Generator(opt).to(device)\n",
        "netG.load_state_dict(torch.load(opt['init_g']))\n",
        "netG.eval()\n",
        "\n",
        "clip_scores = []\n",
        "\n",
        "for img_id in test_images:\n",
        "    # Randomly select 1 of the 5 captions\n",
        "    all_captions = flower_captions[img_id]\n",
        "    selected_idx = random.randint(0, 4)\n",
        "    selected_caption_text = str(all_captions[selected_idx])\n",
        "\n",
        "    # Load corresponding sentence embedding\n",
        "    h = h5py.File(opt['captions_file'])\n",
        "    caption_vector = np.array(h[img_id])[selected_idx][:opt['txt_size']]\n",
        "    h.close()\n",
        "\n",
        "    # Generate noise + embed caption\n",
        "    z = np.random.normal(-1, 1, (1, opt['nz'], 1, 1)).astype(np.float32)\n",
        "    z_tensor = torch.tensor(z, device=device)\n",
        "    caption_tensor = torch.tensor(caption_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_img = netG(z_tensor, caption_tensor)\n",
        "\n",
        "    # Post-process image: (1, 3, H, W) → (H, W, 3) in [0, 255]\n",
        "    fake_img_np = fake_img[0].cpu().numpy()\n",
        "    fake_img_np = ((fake_img_np + 1) * 127.5).astype(np.uint8)\n",
        "    fake_img_np = np.transpose(fake_img_np, (1, 2, 0))\n",
        "\n",
        "    # Convert and preprocess for CLIP\n",
        "    img_clip = clip_transform(fake_img_np).unsqueeze(0).to(device)\n",
        "    text_clip = clip.tokenize([selected_caption_text]).to(device)\n",
        "\n",
        "    # Get CLIP embeddings\n",
        "    with torch.no_grad():\n",
        "        img_feat = clip_model.encode_image(img_clip)\n",
        "        txt_feat = clip_model.encode_text(text_clip)\n",
        "\n",
        "    # Normalize and compute cosine similarity\n",
        "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
        "    similarity = (img_feat @ txt_feat.T).item()\n",
        "    clip_scores.append(similarity)\n",
        "\n",
        "# Final result\n",
        "average_clip_score = np.mean(clip_scores)\n",
        "print(f\"Average CLIP cosine similarity over test set: {average_clip_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcnXtDWstioJ",
        "outputId": "f0826181-5bb1-4262-fe27-52109a8a41bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average CLIP cosine similarity over test set: 0.2869\n"
          ]
        }
      ]
    }
  ]
}